{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO_MpCX84EUU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Dropout, LayerNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Flatten, LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.layers import Bidirectional\n",
        "from math import sqrt\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "plt.style.use('fast')\n",
        "\n",
        "# Hyperparameters\n",
        "sequence_length = 150\n",
        "num_heads = 6\n",
        "ff_dim =50\n",
        "\n",
        "# Create the dataset\n",
        "df = pd.read_csv('Name of the file', index_col='Date', parse_dates=['Date']) #Carga del fichero\n",
        "#Chart parameters are defined, such as index labels, chart size, etc.\n",
        "df.plot(figsize=(15,10))\n",
        "plt.legend(fontsize=10)\n",
        "plt.xlabel('Año', fontsize=25)\n",
        "plt.ylabel('Precio',fontsize=25)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()\n",
        "#The integer value of the size of the dataset is taken and the column of the data is taken\n",
        "v=int(len(df))\n",
        "values = df.iloc[:v,-1].values\n",
        "#We make sure that all values ​​are of type \"Float\"\n",
        "values = values.astype('float32')\n",
        "values=values.reshape(-1, 1)\n",
        "#We divide the data into training, validation and testing\n",
        "n_train_days = int(len(df)*0.8)\n",
        "n_val_days=int(len(df)*0.1)\n",
        "n_test_days=int(len(df)*0.2)\n",
        "train = values[:n_train_days-n_val_days, :]\n",
        "validation= train[:n_val_days, :]\n",
        "test = values[n_train_days:, :]\n",
        "x_train, y_train = train[:, :-1], train[:, -1]\n",
        "x_val, y_val = test[:, :-1], test[:, -1]\n",
        "dataframe1=df.transpose()\n",
        "train_rate = 0.8 #Training rate will determine the learning level of the network\n",
        "seq_len = 50\n",
        "pre_len = 5\n",
        "num_nodes, time_len = dataframe1.shape\n",
        "#Data is normalized between [0,1]\n",
        "def train_test_split(data, train_portion):\n",
        "    time_len = data.shape[1]\n",
        "    train_size = int(time_len * train_portion)\n",
        "    train_data = np.array(data.iloc[:1, :n_train_days-n_val_days])\n",
        "    test_data = np.array(data.iloc[:1, n_train_days:])\n",
        "    val_data =  np.array(data.iloc[:1, n_train_days-n_val_days:n_train_days])\n",
        "    return train_data, test_data,val_data\n",
        "train_data, test_data, val_data = train_test_split(dataframe1, train_rate)\n",
        "def scale_data(train_data, test_data,val_data):\n",
        "    max_speed = train_data.max()\n",
        "    min_speed = train_data.min()\n",
        "    train_scaled = (train_data - min_speed) / (max_speed - min_speed)\n",
        "    test_scaled = (test_data - min_speed) / (max_speed - min_speed)\n",
        "    val_scaled = (val_data - min_speed) / (max_speed - min_speed)\n",
        "    return train_scaled, test_scaled,val_scaled\n",
        "train_scaled, test_scaled, val_scaled = scale_data(train_data, test_data,val_data)\n",
        "def sequence_data_preparation(seq_len, pre_len, train_data, test_data,val_data):\n",
        "    trainX, trainY, testX, testY, valY, valX = [], [], [], [], [], []\n",
        "\n",
        "    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):\n",
        "        a = train_data[:, i : i + seq_len + pre_len]\n",
        "        trainX.append(a[:, :seq_len])\n",
        "        trainY.append(a[:, -1])\n",
        "\n",
        "    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):\n",
        "        b = test_data[:, i : i + seq_len + pre_len]\n",
        "        testX.append(b[:, :seq_len])\n",
        "        testY.append(b[:, -1])\n",
        "\n",
        "    for i in range(val_data.shape[1] - int(seq_len + pre_len - 1)):\n",
        "        c = val_data[:, i : i + seq_len + pre_len]\n",
        "        valX.append(c[:, :seq_len])\n",
        "        valY.append(c[:, -1])\n",
        "\n",
        "    trainX = np.array(trainX)\n",
        "    trainY = np.array(trainY)\n",
        "    testX = np.array(testX)\n",
        "    testY = np.array(testY)\n",
        "    valX = np.array(valX)\n",
        "    valY = np.array(valY)\n",
        "\n",
        "    return trainX, trainY, testX, testY, valX, valY\n",
        "trainX, trainY, testX, testY, valX, valY = sequence_data_preparation(\n",
        "    seq_len, pre_len, train_scaled, test_scaled, val_scaled\n",
        ")\n",
        "\n",
        "\n",
        "# Transformer Model\n",
        "inputs = Input(shape=(1,50))\n",
        "attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=sequence_length)(inputs, inputs)\n",
        "attention_output = Dropout(0.1)(attention_output)\n",
        "skip_connection = tf.keras.layers.Add()([inputs, attention_output])\n",
        "layer_norm1 = LayerNormalization(epsilon=1e-6)(skip_connection)\n",
        "ffn = Dense(ff_dim, activation='relu')(layer_norm1)\n",
        "ffn = Dropout(0.1)(ffn)\n",
        "transformer_output = tf.keras.layers.Add()([layer_norm1, ffn])\n",
        "layer_norm2 = LayerNormalization(epsilon=1e-6)(transformer_output)\n",
        "\n",
        "output = Dense(1, activation='tanh')(tf.keras.layers.Flatten()(layer_norm2))\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "# Train the model with training data\n",
        "model.fit(trainX, trainY, epochs=250, batch_size=250)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "\n",
        "# Select a sample for display\n",
        "sample_index = 25\n",
        "\n",
        "\n",
        "#Data is entered into a dataset to be rescaled\n",
        "df2 = pd.DataFrame(predictions)\n",
        "\n",
        "max_speed = train_data.max()\n",
        "min_speed = train_data.min()\n",
        "train_rescpred = np.array((testY) * max_speed)\n",
        "pred = np.array((predictions) * max_speed)\n",
        "pcompara = pd.DataFrame(np.array([[y[0] for y in train_rescpred], [x[0] for x in pred]])).transpose()\n",
        "\n",
        "\n",
        "pcompara.columns = ['real', 'prediccion']\n",
        "pcompara['diferencia'] = pcompara['real'] - pcompara['prediccion']\n",
        "pcompara.head()\n",
        "pcompara['real'].plot()\n",
        "pcompara['prediccion'].plot()\n",
        "#Los resultados se guardan en un CSV\n",
        "pcompara.to_csv('Name of the file')\n",
        "compara = pd.DataFrame(np.array([y_val, [x[0] for x in predictions]])).transpose()\n",
        "compara.columns = ['real', 'prediccion']\n",
        "# View actual and predicted data\n",
        "plt.plot(testX[sample_index, :, 0], label='Real')\n",
        "plt.plot(predictions[sample_index], label='Predicción')\n",
        "plt.title('Predicción de Serie Temporal con Red Transformer')\n",
        "plt.xlabel('Tiempo')\n",
        "plt.ylabel('Valor')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}